{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from torch.optim import lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_data: pd.DataFrame):\n",
    "        assert 'QuestionTitle' in qa_data.columns, \"DataFrame must contain 'QuestionTitle' column\"\n",
    "        assert 'QuestionBody' in qa_data.columns, \"DataFrame must contain 'QuestionBody' column\"\n",
    "        assert 'Answer' in qa_data.columns, \"DataFrame must contain 'Answer' column\"\n",
    "        \n",
    "        self.qa_data = qa_data\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.qa_data['QuestionTitle'])\n",
    "    \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        # Access the data directly using iloc, which is more memory-efficient\n",
    "        return {\n",
    "            'title': self.qa_data.iloc[index]['QuestionTitle'],\n",
    "            'body': self.qa_data.iloc[index]['QuestionBody'],\n",
    "            'answers': self.qa_data.iloc[index]['Answer']\n",
    "        }\n",
    "        \n",
    "class TrainValidatePipeline:\n",
    "    def __init__(self, q_model, a_model, tokenizer, optimizer, scheduler, device='cpu'):\n",
    "        self.q_model = q_model\n",
    "        self.a_model = a_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "\n",
    "    def tokenize_qa_batch(self, q_titles, q_bodies, answers, max_length=64):\n",
    "        q_batch = self.tokenizer(text = q_titles, text_pair = q_bodies, padding =\"longest\", max_length = max_length, truncation = True, return_tensors =\"pt\")\n",
    "        a_batch = self.tokenizer(text = answers, padding= \"longest\", max_length = max_length, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "        q_batch = {k: v.to(self.device) for k, v in q_batch.items()}\n",
    "        a_batch = {k: v.to(self.device) for k, v in a_batch.items()}\n",
    "\n",
    "        return q_batch, a_batch\n",
    "\n",
    "    def get_class_output(self, model, batch):\n",
    "        output = model(**batch)\n",
    "        output = output.last_hidden_state\n",
    "        return output[:,0,:]\n",
    "\n",
    "    def inbatch_negative_sampling(self, Q, P):\n",
    "        S = (Q @ P.transpose(0,1)).to(self.device)\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "    def contrastive_loss_criterion(self, S, labels=None):\n",
    "        # First Calculate the log softmax as per the paper's definition\n",
    "        softmax_scores = F.log_softmax(S, dim = 1)\n",
    "        if labels == None:\n",
    "            labels = torch.arange(len(S)).to(self.device)\n",
    "\n",
    "        loss = F.nll_loss(softmax_scores, labels.to(self.device))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_topk_indices(self, Q, P, k=None):\n",
    "        S = self.inbatch_negative_sampling(Q, P)\n",
    "        if k == None:\n",
    "            k = len(S)\n",
    "        \n",
    "        scores, indices = torch.topk(S, k)\n",
    "\n",
    "        return indices, scores\n",
    "\n",
    "    def select_by_indices(self, indices, passages):\n",
    "        return [[passages[idx] for idx in index] for index in indices]\n",
    "\n",
    "    def embed_passages(self, passages: 'list[str]', max_length=512):\n",
    "        return self.__embed_text(passages, self.a_model, self.tokenizer, max_length, as_pair=False)\n",
    "\n",
    "    def embed_questions(self, titles, bodies, max_length=512):\n",
    "        return self.__embed_text((titles, bodies), self.q_model, self.tokenizer, max_length, as_pair=True)\n",
    "\n",
    "    def __embed_text(self, texts, model, tokenizer, max_length=512, as_pair=False):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if as_pair:\n",
    "                encoded_batch = tokenizer(\n",
    "                    text=texts[0], text_pair=texts[1],\n",
    "                    max_length=max_length, truncation=True,\n",
    "                    padding='max_length', return_tensors='pt'\n",
    "                )\n",
    "            else:\n",
    "                encoded_batch = tokenizer(\n",
    "                    text=texts,\n",
    "                    max_length=max_length, truncation=True,\n",
    "                    padding='max_length', return_tensors='pt'\n",
    "                )\n",
    "            encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "            outputs = model(**encoded_batch)\n",
    "            return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    def train(self, train_loader, valid_loader, epochs):\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "        validation_recall = []\n",
    "        validation_mrr = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.q_model.train()\n",
    "            self.a_model.train()\n",
    "            total_train_loss = 0\n",
    "\n",
    "            for train_batch in tqdm(train_loader):\n",
    "                q_titles = train_batch['title']\n",
    "                q_bodies = train_batch['body']\n",
    "                answers = train_batch['answers']\n",
    "\n",
    "                # Tokenize and embed the batch data\n",
    "                q_batch, a_batch = self.tokenize_qa_batch(q_titles, q_bodies, answers)\n",
    "                q_out = self.get_class_output(self.q_model, q_batch)\n",
    "                a_out = self.get_class_output(self.a_model, a_batch)\n",
    "\n",
    "                S = self.inbatch_negative_sampling(q_out, a_out)\n",
    "                loss = self.contrastive_loss_criterion(S)\n",
    "                total_train_loss += loss.item()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            average_train_loss = total_train_loss / len(train_loader)\n",
    "            print(f\"Epoch: {epoch+1} | Average training loss per sample: {average_train_loss}\")\n",
    "            training_loss.append(average_train_loss)\n",
    "\n",
    "            # Perform validation\n",
    "            avg_valid_loss, recall_k, mrr = self.validate(valid_loader)\n",
    "            validation_loss.append(avg_valid_loss)\n",
    "            validation_recall.append(recall_k)\n",
    "            validation_mrr.append(mrr)\n",
    "\n",
    "        return training_loss, validation_loss, validation_recall, validation_mrr\n",
    "\n",
    "    def validate(self, valid_loader):\n",
    "        self.q_model.eval()\n",
    "        self.a_model.eval()\n",
    "        total_valid_loss = 0\n",
    "        all_retrieved_indices = []\n",
    "        all_true_indices = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for valid_batch in tqdm(valid_loader):\n",
    "                q_titles = valid_batch['title']\n",
    "                q_bodies = valid_batch['body']\n",
    "                answers = valid_batch['answers']\n",
    "\n",
    "                # Embed questions and answers\n",
    "                Q = self.embed_questions(titles=q_titles, bodies=q_bodies, max_length=512)\n",
    "                P = self.embed_passages(passages=answers, max_length=512)\n",
    "\n",
    "                S = self.inbatch_negative_sampling(Q, P)\n",
    "                loss = self.contrastive_loss_criterion(S)\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                indices, _ = self.get_topk_indices(Q, P, k=5)\n",
    "                true_indices = list(range(len(Q)))\n",
    "                all_retrieved_indices.extend(indices.cpu().tolist())\n",
    "                all_true_indices.extend(true_indices)\n",
    "\n",
    "        average_valid_loss = total_valid_loss / len(valid_loader)\n",
    "        recall_k = self.recall_at_k(all_retrieved_indices, all_true_indices, k=5)\n",
    "        mrr = self.mean_reciprocal_rank(all_retrieved_indices, all_true_indices)\n",
    "\n",
    "        print(f\"Validation | Average loss per sample: {average_valid_loss}\")\n",
    "        print(f\"Validation | Recall@k: {recall_k}\")\n",
    "        print(f\"Validation | MRR: {mrr}\")\n",
    "\n",
    "        return average_valid_loss, recall_k, mrr\n",
    "    \n",
    "    def recall_at_k(self, retrieved_indices, true_indices, k):\n",
    "        hit = 0\n",
    "        for true,retrieved in zip(true_indices, retrieved_indices):\n",
    "            top_k_set = set(retrieved[:k])\n",
    "            if true in top_k_set:\n",
    "                hit += 1\n",
    "        total = len(true_indices)\n",
    "\n",
    "        return hit / total\n",
    "\n",
    "    def mean_reciprocal_rank(self, retrieved_indices, true_indices):\n",
    "        hit = 0\n",
    "        for true,retrieved in zip(true_indices, retrieved_indices):\n",
    "            try:\n",
    "                rank = 1 + retrieved.index(true)\n",
    "                hit += 1 / rank\n",
    "            except ValueError:\n",
    "                continue\n",
    "        total = len(true_indices)\n",
    "        return hit / total\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_models_and_tokenizer(q_name, a_name, t_name, device='cpu'):\n",
    "    q_enc = transformers.AutoModel.from_pretrained(q_name).to(device)\n",
    "    a_enc = transformers.AutoModel.from_pretrained(a_name).to(device)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(t_name)\n",
    "    \n",
    "    return q_enc, a_enc, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Reproducibility\n",
    "random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/.local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "  0%|          | 0/169 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "bsize = 64\n",
    "n_epoch = 10\n",
    "lr = 5e-5\n",
    "name = 'google/electra-small-discriminator'\n",
    "step_size = 8\n",
    "gamma = 0.8\n",
    "\n",
    "# Load File\n",
    "qa_data = dict(\n",
    "    train = pd.read_csv('qa/train.csv'),\n",
    "    valid = pd.read_csv('qa/validation.csv'),\n",
    "    answers = pd.read_csv('qa/answers.csv'),\n",
    "    test = pd.read_csv('qa/test.csv'),\n",
    ")\n",
    "\n",
    "train_dataset = QADataset(qa_data['train'])\n",
    "valid_dataset = QADataset(qa_data['valid'])                      \n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = bsize, shuffle = True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = bsize, shuffle = False)\n",
    "\n",
    "q_enc, a_enc, tokenizer = load_models_and_tokenizer(q_name=name, a_name=name, t_name=name, device = device)\n",
    "optimizer = torch.optim.Adam(chain(q_enc.parameters(), a_enc.parameters()), lr= lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer,step_size = step_size, gamma = gamma)\n",
    "\n",
    "pipeline = TrainValidatePipeline(q_enc, a_enc, tokenizer, optimizer, scheduler, device)\n",
    "\n",
    "t_l, v_l, v_r, v_mrr = pipeline.train(train_loader, valid_loader, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv('/kaggle/input/assignment4/data/qa/answers.csv')\n",
    "def load_model(model_path, model_name, device = 'cpu'):\n",
    "    model =  transformers.AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location = device))\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_name):\n",
    "    return transformers.AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "load_a = load_model(\"/kaggle/input/bert-model-odqa/a_encoder_model.bin\", 'google/electra-small-discriminator', device )\n",
    "load_q = load_model(\"/kaggle/input/bert-model-odqa/q_encoder_model.bin\", 'google/electra-small-discriminator', device )\n",
    "tokenizer = load_tokenizer('google/electra-small-discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import get_default_qconfig\n",
    "from torch.quantization import prepare\n",
    "\n",
    "qconfig = get_default_qconfig('fbgemm') \n",
    "load_a.eval()\n",
    "load_q.eval()\n",
    "load_a.qconfig = qconfig\n",
    "load_q.qconfig = qconfig\n",
    "\n",
    "prepare(load_q, inplace=True)\n",
    "prepare(load_a, inplace=True)\n",
    "\n",
    "load_a = nn.DataParallel(load_a)\n",
    "load_q = nn.DataParallel(load_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline:\n",
    "    def __init__(self, q_model, a_model, tokenizer,answer_loader, answer, device = \"cpu\"):\n",
    "        self.q_model = q_model\n",
    "        self.a_model = a_model\n",
    "        self.device = device \n",
    "        self.tokenizer = tokenizer \n",
    "        self.answer_loader = answer_loader\n",
    "        self.answer = answer\n",
    "        self.output_embedding = None\n",
    "    \n",
    "    def embed_passage(self, batch_size = 64, max_length = 256):\n",
    "        self.a_model.eval()\n",
    "        # Process answers in batches\n",
    "        output_embedding = torch.zeros((350, 256), device=self.device)\n",
    "        position = 0\n",
    "        for answers in self.answer_loader:\n",
    "            with torch.no_grad():\n",
    "                encoded_batch = self.tokenizer(\n",
    "                    text = answers,\n",
    "                    max_length = max_length,\n",
    "                    truncation = True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "            encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "            outputs = self.a_model(**encoded_batch)\n",
    "            batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "            \n",
    "            batch_size = batch_embedding.size(0)\n",
    "            output_embedding[position : position + batch_size, :] = batch_embedding\n",
    "            \n",
    "            position += batch_size  \n",
    "        self.output_embedding = output_embedding\n",
    "        return output_embedding\n",
    "    \n",
    "    def embed_question(self, title, body, max_length = 100):\n",
    "        self.q_model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_batch = self.tokenizer(\n",
    "                text=title, text_pair=body,\n",
    "                max_length=max_length, truncation=True,\n",
    "                padding='max_length', return_tensors='pt' \n",
    "            )\n",
    "        encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "        outputs = self.q_model(**encoded_batch)\n",
    "        batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "        return batch_embedding\n",
    "    \n",
    "    def inbatch_negative_sampling(self, Q, P):\n",
    "        S = (Q @ P.transpose(0,1)).to(self.device)\n",
    "        return S\n",
    "\n",
    "\n",
    "    def get_topk_indices(self, Q, P, k=None):\n",
    "        S = self.inbatch_negative_sampling(Q, P)\n",
    "        if k == None:\n",
    "            k = len(S)\n",
    "        scores, indices = torch.topk(S, k)\n",
    "\n",
    "        return indices, scores\n",
    "\n",
    "    def inference(self, title, body):\n",
    "        Q = self.embed_question(title, body)\n",
    "        if self.output_embedding is None:\n",
    "            P = self.embed_passage()\n",
    "        else:\n",
    "            P = self.output_embedding\n",
    "        idx, scores = self.get_topk_indices(Q, P, k = 1)\n",
    "        return self.answer[idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerDataset(Dataset):\n",
    "    def __init__(self, answer):\n",
    "        self.answer = answer[:350]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.answer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.answer[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "answers['Answer'] = answers['Answer'].fillna('').str.replace('[^a-zA-Z0-9.!,]', ' ', regex=True).replace('\\s+', ' ', regex=True)\n",
    "answer_full = answers['Answer'].tolist()\n",
    "answers['Answer'] = answers['Answer'].str[:250]\n",
    "answer = answers['Answer'].tolist()\n",
    "\n",
    "answerDataset = AnswerDataset(answer)\n",
    "answer_loader = DataLoader(answerDataset, batch_size = batch_size, shuffle = False, num_workers = 4, pin_memory = True)\n",
    "testpipeline = InferencePipeline(load_q, load_a, tokenizer,answer_loader, answer_full, device)\n",
    "title = [\"Season Fried Chicken\"]\n",
    "body = [\"I've been trying to season fried chicken but I don't really know where to start.\"]\n",
    "testpipeline.inference(title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_database():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
