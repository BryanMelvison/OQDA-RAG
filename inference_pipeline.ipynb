{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Tuple, List, Union\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from collections import OrderedDict\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Reproducibility\n",
    "random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def enableMultiGPU(model: AutoModel, multi_gpu: bool):\n",
    "    if multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "    return model\n",
    "\n",
    "# To Load our weights to the model (since our model was trained on multiple gpus, we have to do string processing)\n",
    "def load_model_from_gpu(model_path: str, model_name: str, device : torch.device = 'cpu'):\n",
    "\n",
    "    our_state_dict = torch.load(model_path, map_location = device)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in our_state_dict.items():\n",
    "        name = k[7:] if k.startswith('module.') else k  # Remove the 'module.' prefix\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the state dict into your model\n",
    "    model = AutoModel.from_pretrained(model_name, state_dict=new_state_dict).to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(model_path: str, model_name: str, device: torch.device = 'cpu'):\n",
    "    model =  transformers.AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location = device))\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_name: str):\n",
    "    return transformers.AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpu = False\n",
    "a_name = 'google/electra-small-discriminator'\n",
    "q_name = 'google/electra-small-discriminator'\n",
    "t_name = 'google/electra-small-discriminator'\n",
    "a_path = \"model/a_encoder_model.bin\"\n",
    "q_path = \"model/q_encoder_model.bin\"\n",
    "\n",
    "# Load Passage, Models and Tokenizers:\n",
    "answers = pd.read_csv('qa/answers.csv')\n",
    "\n",
    "# load_a = load_model(a_path, a_name , device )\n",
    "# load_q = load_model(q_path, q_name, device )\n",
    "\n",
    "load_a = load_model_from_gpu(a_path, a_name, device)\n",
    "load_q = load_model_from_gpu(q_path, q_name, device)\n",
    "\n",
    "a_enc = enableMultiGPU(load_a, multi_gpu)\n",
    "q_enc = enableMultiGPU(load_q, multi_gpu)\n",
    "\n",
    "tokenizer = load_tokenizer(t_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline:\n",
    "    def __init__(self, q_model: AutoModel, a_model: AutoModel, tokenizer: AutoTokenizer, answer_loader: DataLoader, answer: List[str], device: torch.device = \"cpu\"):\n",
    "        self.q_model = q_model\n",
    "        self.a_model = a_model\n",
    "        self.device = device \n",
    "        self.tokenizer = tokenizer \n",
    "        self.answer_loader = answer_loader\n",
    "        self.answer = answer\n",
    "        self.database = None\n",
    "    \n",
    "    # Embed the entire answer corpus, and post them to a MongoDB database.\n",
    "    def embed_passage(self, max_length: int = 512):\n",
    "        self.a_model.eval()\n",
    "        # Process answers in batches\n",
    "        global_idx = 0\n",
    "        for answers in self.answer_loader:\n",
    "            with torch.no_grad():\n",
    "                encoded_batch = self.tokenizer(\n",
    "                    text = answers,\n",
    "                    max_length = max_length,\n",
    "                    truncation = True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "                encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "                outputs = self.a_model(**encoded_batch)\n",
    "                batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "                #Insert all the output in a batch into the DataBase\n",
    "                embeddings_list = []\n",
    "                for embedding in batch_embedding:\n",
    "                    embedding_id = global_idx\n",
    "                    embedding_list = embedding.cpu().numpy().tolist()\n",
    "                    document = {\n",
    "                        '_id': embedding_id,\n",
    "                        'embedding': embedding_list\n",
    "                    }\n",
    "                    embeddings_list.append(document)\n",
    "                    global_idx += 1\n",
    "                self.database_manager.insert_embeddings_batch(embeddings_list)\n",
    "            \n",
    "    \n",
    "    def embed_question(self, title: List[str], body: List[str], max_length: int = 512) -> BaseModelOutput:\n",
    "        self.q_model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_batch = self.tokenizer(\n",
    "                text=title, text_pair=body,\n",
    "                max_length=max_length, truncation=True,\n",
    "                padding='max_length', return_tensors='pt' \n",
    "            )\n",
    "        encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "        outputs = self.q_model(**encoded_batch)\n",
    "        batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "        return batch_embedding\n",
    "    \n",
    "    def inbatch_negative_sampling(self, Q: Tensor, P: Tensor) -> Tensor:\n",
    "        S = (Q @ P.transpose(0,1)).to(self.device)\n",
    "        return S\n",
    "\n",
    "\n",
    "    def get_topk_indices(self, Q: Tensor, P: Tensor, k: int=None) -> Tuple[Tensor, Tensor]:\n",
    "        S = self.inbatch_negative_sampling(Q, P)\n",
    "        if k == None:\n",
    "            k = len(S)\n",
    "        scores, indices = torch.topk(S, k)\n",
    "\n",
    "        return indices, scores\n",
    "\n",
    "    \n",
    "    def inference(self, title: List[str], body: List[str]) -> List[List[str]]:\n",
    "        Q = self.embed_question(title, body)\n",
    "        P = self.database_manager.load_embeddings().to(self.device)\n",
    "        idx, scores = self.get_topk_indices(Q, P, k = 2)\n",
    "        idx.squeeze_(0)\n",
    "        return [self.answer[ix] for ix in idx]\n",
    "    \n",
    "    def connection_db(self, database_manager):\n",
    "        self.database_manager = database_manager\n",
    "        self.database_manager.establish_connection()\n",
    "\n",
    "    def disconnect_db(self):\n",
    "        try:\n",
    "            self.database_manager.close_connection()\n",
    "        except NameError as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = os.getenv(\"DATABASE_NAME\")\n",
    "collection = os.getenv(\"COLLECTION_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self, database: str, collection: str):\n",
    "\n",
    "        self.database_name = database\n",
    "        self.collection_name = collection\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "        \n",
    "    def establish_connection(self):\n",
    "        # Use environment variables for sensitive information\n",
    "        user = os.getenv('MONGO_USERNAME')\n",
    "        pw = os.getenv('MONGO_PASSWORD')\n",
    "        link = os.getenv(\"MONGO_LINK\")\n",
    "        \n",
    "        CONNECTION_STRING = f\"mongodb+srv://{user}:{pw}@{link}\"\n",
    "        \n",
    "        try: \n",
    "            self.client = MongoClient(CONNECTION_STRING)\n",
    "            self.db = self.client[self.database_name]\n",
    "            self.collection = self.db[self.collection_name]\n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            print(f\"Error connecting to Database: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def insert_embeddings_batch(self, embeddings: List[float]):\n",
    "        if not self.collection:\n",
    "            raise Exception(\"Database connection is not established.\")\n",
    "        try:\n",
    "            self.collection.insert_many(embeddings, ordered=False)\n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            print(f\"Error inserting embeddings batch: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def load_embeddings(self) -> Tensor:\n",
    "        embeddings = []\n",
    "        for doc in self.collection.find({}, {'_id': 0, 'embedding': 1}):\n",
    "            embeddings.append(doc['embedding'])\n",
    "        return torch.tensor(embeddings, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    def close_connection(self):\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            self.client = None\n",
    "            self.db = None\n",
    "            self.collection = None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerDataset(Dataset):\n",
    "    def __init__(self, answer: List[str]):\n",
    "        self.answer = answer\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.answer)\n",
    "    \n",
    "    def __getitem__(self, index) -> str:\n",
    "        return self.answer[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "answers['Answer'] = answers['Answer'].fillna('').str.replace('[^a-zA-Z0-9.!,]', ' ', regex=True).replace('\\s+', ' ', regex=True)\n",
    "answer_full = answers['Answer'].tolist()\n",
    "answer = answers['Answer'].tolist()\n",
    "\n",
    "answerDataset = AnswerDataset(answer)\n",
    "answer_loader = DataLoader(answerDataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpipeline = InferencePipeline(q_enc, a_enc, tokenizer,answer_loader, answer_full, device)\n",
    "database_manager = DatabaseManager(database, collection)\n",
    "testpipeline.connection_db(database_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mine were big and difficult to cut so I put them in the microwave for a few minutes to soften them. After cubing the potatoes yams, I boiled them as usual, to make mashed sweet potatoes. After the microwaving, it only took about 15 20 minutes to boil enough to soften. ',\n",
       " 'No, they are 2 different things. How are you preparing them Have you tried different ways of cooking them boiled, roasted, mashed, fried... You could try mixing them up with regular potatoes. When making mashed potatoes, add some of the sweet potatoes to the mix, start with a little bit, then increase ratio of sweet to regular potatoes. ']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title1 = \"Making Creamy Mashed Potatoes\"\n",
    "body1 = \"I want to make creamy mashed potatoes, but they always come out too lumpy or dry. What's the secret to getting them smooth and creamy?\"\n",
    "title = [title1]\n",
    "body = [body1]\n",
    "testpipeline.inference(title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Database Connection:\n",
    "testpipeline.database_manager.close_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
