{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Tuple, List, Union\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from collections import OrderedDict\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Reproducibility\n",
    "random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def enableMultiGPU(model: AutoModel, multi_gpu: bool):\n",
    "    if multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "    return model\n",
    "\n",
    "# To Load our weights to the model (since our model was trained on multiple gpus, we have to do string processing)\n",
    "def load_model_from_gpu(model_path: str, model_name: str, device : torch.device = 'cpu'):\n",
    "\n",
    "    our_state_dict = torch.load(model_path, map_location = device)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in our_state_dict.items():\n",
    "        name = k[7:] if k.startswith('module.') else k  # Remove the 'module.' prefix\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # Load the state dict into your model\n",
    "    model = AutoModel.from_pretrained(model_name, state_dict=new_state_dict).to(device)\n",
    "    return model\n",
    "\n",
    "def load_model(model_path: str, model_name: str, device: torch.device = 'cpu'):\n",
    "    model =  transformers.AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location = device))\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_name: str):\n",
    "    return transformers.AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpu = False\n",
    "a_name = 'google/electra-small-discriminator'\n",
    "q_name = 'google/electra-small-discriminator'\n",
    "t_name = 'google/electra-small-discriminator'\n",
    "a_path = \"model/a_encoder_model.bin\"\n",
    "q_path = \"model/q_encoder_model.bin\"\n",
    "\n",
    "# Load Passage, Models and Tokenizers:\n",
    "answers = pd.read_csv('qa/answers.csv')\n",
    "\n",
    "# load_a = load_model(a_path, a_name , device )\n",
    "# load_q = load_model(q_path, q_name, device )\n",
    "\n",
    "load_a = load_model_from_gpu(a_path, a_name, device)\n",
    "load_q = load_model_from_gpu(q_path, q_name, device)\n",
    "\n",
    "a_enc = enableMultiGPU(load_a, multi_gpu)\n",
    "q_enc = enableMultiGPU(load_q, multi_gpu)\n",
    "\n",
    "tokenizer = load_tokenizer(t_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline:\n",
    "    def __init__(self, q_model: AutoModel, a_model: AutoModel, tokenizer: AutoTokenizer, answer_loader: DataLoader, device: torch.device = \"cpu\"):\n",
    "        self.q_model = q_model\n",
    "        self.a_model = a_model\n",
    "        self.device = device \n",
    "        self.tokenizer = tokenizer \n",
    "        self.answer_loader = answer_loader\n",
    "    \n",
    "    # Embed the entire answer corpus, and post them to a MongoDB database.\n",
    "    def embed_passage(self, max_length: int = 512):\n",
    "        self.a_model.eval()\n",
    "        # Process answers in batchesclass DatabasePipeline:\n",
    "    def __init__(self, q_model: AutoModel, a_model: AutoModel, tokenizer: AutoTokenizer, answer_loader: DataLoader, device: torch.device = \"cpu\"):\n",
    "        self.q_model = q_model\n",
    "        self.a_model = a_model\n",
    "        self.device = device \n",
    "        self.tokenizer = tokenizer \n",
    "        self.answer_loader = answer_loader\n",
    "    \n",
    "    # Embed the entire answer corpus, and post them to a MongoDB database.\n",
    "    def embed_passage(self, max_length: int = 512):\n",
    "        self.a_model.eval()\n",
    "        # Process answers in batches\n",
    "        global_idx = 0\n",
    "        for answers in self.answer_loader:\n",
    "            with torch.no_grad():\n",
    "                encoded_batch = self.tokenizer(\n",
    "                    text = answers,\n",
    "                    max_length = max_length,\n",
    "                    truncation = True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "                encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "                outputs = self.a_model(**encoded_batch)\n",
    "                batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "                #Insert all the output in a batch into the DataBase\n",
    "                embeddings_list = []\n",
    "                for embedding in batch_embedding:\n",
    "                    embedding_id = global_idx\n",
    "                    embedding_list = embedding.cpu().numpy().tolist()\n",
    "                    document = {\n",
    "                        '_id': embedding_id,\n",
    "                        'embedding': embedding_list\n",
    "                    }\n",
    "                    embeddings_list.append(document)\n",
    "                    global_idx += 1\n",
    "                self.database_manager_embedding.insert_embeddings_batch(embeddings_list)\n",
    "            \n",
    "\n",
    "    def insertanswersdb(self):\n",
    "        index = 0\n",
    "        for ans in self.answer_loader:\n",
    "            ans_list = []\n",
    "            for a in ans:\n",
    "                ans_id = index\n",
    "                ans_lst = a\n",
    "                document = {\n",
    "                    '_id': f\"answer_{ans_id}\",\n",
    "                    'ans_list': ans_lst\n",
    "                }\n",
    "                ans_list.append(document)\n",
    "                index += 1\n",
    "            self.database_manager_answer.insert_embeddings_batch(ans_list)    \n",
    "    \n",
    "    def connection_db(self, database_manager_embedding, database_manager_answer):\n",
    "        self.database_manager_embedding = database_manager_embedding\n",
    "        self.database_manager_answer = database_manager_answer\n",
    "\n",
    "        self.database_manager_embedding.establish_connection()\n",
    "        self.database_manager_answer.establish_connection()\n",
    "\n",
    "\n",
    "    def disconnect_db(self):\n",
    "        try:\n",
    "            self.database_manager_embedding.close_connection()\n",
    "            self.database_manager_answer.close_connection()\n",
    "        except NameError as e:\n",
    "            print(f\"An error occurred: {e}\") \n",
    "        global_idx = 0\n",
    "        for answers in self.answer_loader:\n",
    "            with torch.no_grad():\n",
    "                encoded_batch = self.tokenizer(\n",
    "                    text = answers,\n",
    "                    max_length = max_length,\n",
    "                    truncation = True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors = 'pt'\n",
    "                )\n",
    "                encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "                outputs = self.a_model(**encoded_batch)\n",
    "                batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "                #Insert all the output in a batch into the DataBase\n",
    "                embeddings_list = []\n",
    "                for embedding in batch_embedding:\n",
    "                    embedding_id = global_idx\n",
    "                    embedding_list = embedding.cpu().numpy().tolist()\n",
    "                    document = {\n",
    "                        '_id': embedding_id,\n",
    "                        'embedding': embedding_list\n",
    "                    }\n",
    "                    embeddings_list.append(document)\n",
    "                    global_idx += 1\n",
    "                self.database_manager_embedding.insert_embeddings_batch(embeddings_list)\n",
    "            \n",
    "    \n",
    "    def embed_question(self, title: List[str], body: List[str], max_length: int = 512) -> BaseModelOutput:\n",
    "        self.q_model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_batch = self.tokenizer(\n",
    "                text=title, text_pair=body,\n",
    "                max_length=max_length, truncation=True,\n",
    "                padding='max_length', return_tensors='pt' \n",
    "            )\n",
    "        encoded_batch = {k: v.to(self.device) for k, v in encoded_batch.items()}\n",
    "        outputs = self.q_model(**encoded_batch)\n",
    "        batch_embedding = outputs.last_hidden_state[:,0,:]\n",
    "        return batch_embedding\n",
    "    \n",
    "    def inbatch_negative_sampling(self, Q: Tensor, P: Tensor) -> Tensor:\n",
    "        S = (Q @ P.transpose(0,1)).to(self.device)\n",
    "        return S\n",
    "\n",
    "\n",
    "    def get_topk_indices(self, Q: Tensor, P: Tensor, k: int=None) -> Tuple[Tensor, Tensor]:\n",
    "        S = self.inbatch_negative_sampling(Q, P)\n",
    "        if k == None:\n",
    "            k = len(S)\n",
    "        scores, indices = torch.topk(S, k)\n",
    "\n",
    "        return indices, scores\n",
    "\n",
    "    def insertanswersdb(self):\n",
    "        index = 0\n",
    "        for ans in self.answer_loader:\n",
    "            ans_list = []\n",
    "            for a in ans:\n",
    "                ans_id = index\n",
    "                ans_lst = a\n",
    "                document = {\n",
    "                    '_id': f\"answer_{ans_id}\",\n",
    "                    'ans_list': ans_lst\n",
    "                }\n",
    "                ans_list.append(document)\n",
    "                index += 1\n",
    "            self.database_manager_answer.insert_embeddings_batch(ans_list)    \n",
    "\n",
    "    def inference(self, title: List[str], body: List[str]) -> List[List[str]]:\n",
    "        Q = self.embed_question(title, body)\n",
    "        P = self.database_manager_embedding.load_embeddings().to(self.device)\n",
    "        idx, scores = self.get_topk_indices(Q, P, k = 5)\n",
    "        idx.squeeze_(0)\n",
    "        return [self.database_manager_answer.find_element(ix) for ix in idx]\n",
    "\n",
    "    \n",
    "    def connection_db(self, database_manager_embedding, database_manager_answer):\n",
    "        self.database_manager_embedding = database_manager_embedding\n",
    "        self.database_manager_answer = database_manager_answer\n",
    "\n",
    "        self.database_manager_embedding.establish_connection()\n",
    "        self.database_manager_answer.establish_connection()\n",
    "\n",
    "\n",
    "    def disconnect_db(self):\n",
    "        try:\n",
    "            self.database_manager_embedding.close_connection()\n",
    "            self.database_manager_answer.close_connection()\n",
    "        except NameError as e:\n",
    "            print(f\"An error occurred: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages\n"
     ]
    }
   ],
   "source": [
    "database = os.getenv(\"DATABASE_NAME\")\n",
    "collection = os.getenv(\"COLLECTION_NAME\")\n",
    "\n",
    "database_1 = os.getenv(\"DATABASE_NAME_1\")\n",
    "collection_1 = os.getenv(\"COLLECTION_NAME_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    def __init__(self, database: str, collection: str):\n",
    "\n",
    "        self.database_name = database\n",
    "        self.collection_name = collection\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "        \n",
    "    def establish_connection(self):\n",
    "        # Use environment variables for sensitive information\n",
    "        user = os.getenv('MONGO_USERNAME')\n",
    "        pw = os.getenv('MONGO_PASSWORD')\n",
    "        link = os.getenv(\"MONGO_LINK\")\n",
    "        \n",
    "        CONNECTION_STRING = f\"mongodb+srv://{user}:{pw}@{link}\"\n",
    "        \n",
    "        try: \n",
    "            self.client = MongoClient(CONNECTION_STRING)\n",
    "            self.db = self.client[self.database_name]\n",
    "            self.collection = self.db[self.collection_name]\n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            print(f\"Error connecting to Database: {e}\")\n",
    "            raise\n",
    "        \n",
    "    def insert_embeddings_batch(self, embeddings: List[float]):\n",
    "        if not self.collection:\n",
    "            raise Exception(\"Database connection is not established.\")\n",
    "        try:\n",
    "            self.collection.insert_many(embeddings, ordered=False)\n",
    "        except Exception as e:\n",
    "            # Log the error\n",
    "            print(f\"Error inserting embeddings batch: {e}\")\n",
    "            raise\n",
    "            \n",
    "    \n",
    "    def load_embeddings(self) -> Tensor:\n",
    "        embeddings = []\n",
    "        for doc in self.collection.find({}, {'_id': 0, 'embedding': 1}):\n",
    "            embeddings.append(doc['embedding'])\n",
    "        return torch.tensor(embeddings, dtype=torch.float32)\n",
    "    \n",
    "    def find_element(self, idx: int) -> List:\n",
    "        ans = self.collection.find_one({'_id': f\"answer_{idx}\"}, {'_id': 0, 'ans_list': 1})\n",
    "        # Check if a document was found\n",
    "        if ans:\n",
    "            return ans['ans_list']\n",
    "        else:\n",
    "            # Handle the case where no document is found\n",
    "            return None  \n",
    "    \n",
    "    def close_connection(self):\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            self.client = None\n",
    "            self.db = None\n",
    "            self.collection = None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerDataset(Dataset):\n",
    "    def __init__(self, answer: List[str]):\n",
    "        self.answer = answer\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.answer)\n",
    "    \n",
    "    def __getitem__(self, index) -> str:\n",
    "        return self.answer[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "answers['Answer'] = answers['Answer'].fillna('').str.replace('[^a-zA-Z0-9.!,]', ' ', regex=True).replace('\\s+', ' ', regex=True)\n",
    "answer = answers['Answer'].tolist()\n",
    "\n",
    "answerDataset = AnswerDataset(answer)\n",
    "answer_loader = DataLoader(answerDataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpipeline = InferencePipeline(q_enc, a_enc, tokenizer,answer_loader, device)\n",
    "database_manager = DatabaseManager(database, collection)\n",
    "answer_database_manager = DatabaseManager(database_1, collection_1)\n",
    "\n",
    "testpipeline.connection_db(database_manager, answer_database_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I would advise against it. Cooking in acid makes vegetables firm. Sometimes this is a good thing, but if you want to mash your potatoes afterwards, you want them soft. Else you ll get the wrong texture. There are numerous recipes for adding dairy products to the mashed potatoes after they have been cooked and mashed, and they taste well. You can look for them, or experiment yourself. It s a matter of what taste texture you like, there aren t any physics involved which you d throw off with a wrong ratio. ',\n",
       " 'Consider instant mashed potatoes. Mashed potatoes from dried potato flakes are a lot better than most people given them credit for, and would probably be superior to real mashed potatoes made with poor tools in a hurry. More importantly for you, the process of cooking them scales up to any reasonable quantity you just add the correct ratio of flakes, butter, and milk https idahoan.com products idahoan original mashed potatoes 26oz on the stove and you can make up to 8 liters at a time. If the chef isn t OK with that, then he should buy you the equipment to do better. ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title1 = \"Making Creamy Mashed Potatoes\"\n",
    "body1 = \"I want to make creamy mashed potatoes, but they always come out too lumpy or dry. What's the secret to getting them smooth and creamy?\"\n",
    "title = [title1]\n",
    "body = [body1]\n",
    "testpipeline.inference(title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Database Connection:\n",
    "testpipeline.disconnect_db()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
